{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Writing Like Shakespeare using Deep Learning</h1>\n",
    "-Kashish\n",
    "<h3>Problem Statement</h3>\n",
    "We have a collection of Shakespeare' Sonnets, we want to build a Deep Learning System using a deep Recurrent Neural Network with a combination of Bidirectional-LSTMs and LSTMs that could help us generate similar poems.\n",
    "\n",
    "<h3>Real-world/Business-objectives and constraints:</h3>\n",
    "\n",
    "1. Minimize multi-class error.\n",
    "2. Multi-class probability estimates.\n",
    "3. Generating output should not take hours and block system resources. It should be efficient enough to work in a few seconds.\n",
    "\n",
    "- All necessary imports are made when required and specific functions/attributes only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Collecting the Data</h3>\n",
    "<h4>Data</h4>\n",
    "\n",
    "Our dataset contains a collection of Shakespeare' Sonnets, downloaded from the following link :\n",
    "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
    "\n",
    "<h4>Data Sample</h4>\n",
    "\n",
    "Shall sum my count and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\n",
    "Look in thy glass, and tell the face thou viewest\n",
    "Now is the time that face should form another;\n",
    "Whose fresh repair if now thou not renewest,\n",
    "Thou dost beguile the world, unbless some mother.\n",
    "For where is she so fair whose unear'd womb\n",
    "Disdains the tillage of thy husbandry?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-01-31 18:36:44--  http://%5C/\n",
      "Resolving \\\\ (\\\\)... failed: No such host is known. .\n",
      "wget: unable to resolve host address '\\\\'\n",
      "--2022-01-31 18:36:44--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.193.80, 142.250.207.208, 142.250.193.208, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.193.80|:443... connected.\n",
      "WARNING: cannot verify storage.googleapis.com's certificate, issued by 'CN=GTS CA 1C3,O=Google Trust Services LLC,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93578 (91K) [text/plain]\n",
      "Saving to: 'sonnets.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 54% 1.01M 0s\n",
      "    50K .......... .......... .......... .......... .         100% 1.21M=0.08s\n",
      "\n",
      "2022-01-31 18:36:47 (1.09 MB/s) - 'sonnets.txt' saved [93578/93578]\n",
      "\n",
      "--2022-01-31 18:36:47--  http://%5C/\n",
      "Resolving \\\\ (\\\\)... failed: No such host is known. .\n",
      "wget: unable to resolve host address '\\\\'\n",
      "FINISHED --2022-01-31 18:36:47--\n",
      "Total wall clock time: 2.8s\n",
      "Downloaded: 1 files, 91K in 0.08s (1.09 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\ https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\ -O sonnets.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Exploratory data Analysis</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in dataset: 2158\n",
      "Number of unique words in dataset: 3210\n",
      "Max number of words in a sentence: 11\n",
      "Min number of words in a sentence: 5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import data\n",
    "\n",
    "dataset = data.TextLineDataset(\"sonnets.txt\")\n",
    "\n",
    "# count no. of lines in the dataset\n",
    "raw_data = [element.decode(\"utf-8\") for element in dataset.as_numpy_iterator()]\n",
    "print(\"Number of lines in dataset: %s\" % len(raw_data))\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# count no. of unique words in dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_data)\n",
    "print(\"Number of unique words in dataset: %s\" % len(tokenizer.word_index))\n",
    "# find max and min no. of words in a sentence\n",
    "tokenized_data = tokenizer.texts_to_sequences(raw_data)\n",
    "lines_length = [len(sequence) for sequence in tokenized_data if len(sequence) > 1]\n",
    "print(\n",
    "    \"Max number of words in a sentence: {}\\nMin number of words in a sentence: {}\".format(\n",
    "        max(lines_length), min(lines_length)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot a graph\n",
    "# from seaborn import displot\n",
    "# from pandas import DataFrame\n",
    "# from matplotlib.pyplot import grid\n",
    "\n",
    "# temp_df=DataFrame({'n_lines':lines_length})\n",
    "# temp_df=temp_df.n_lines.value_counts().reset_index()\n",
    "# displot(lines_length,kind='hist',color='crimson',aspect=1,height=5)\n",
    "# grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare the data to feed to model for training</h3>\n",
    "Steps:\n",
    "\n",
    "1. Tokenizing the data, using a word Tokenizer.\n",
    "2. Create n-gram sequences.\n",
    "3. Pre-padding the sequences to make them of equal lengths.\n",
    "4. Splitting the last word from each sentence to form label.\n",
    "\n",
    "For example:\n",
    "\n",
    "<li> Sentence: 'FROM fairest creatures we desire increase'</li><br>\n",
    "<li> Step-1: [34, 417, 877, 166, 213, 517]</li><br>\n",
    "<li> Step-2: Create n-gram sequences<br>&emsp;[[34, 417],\n",
    " <br>&emsp;[34, 417, 877],\n",
    " <br>&emsp;[34, 417, 877, 166],\n",
    " <br>&emsp;[34, 417, 877, 166, 213],\n",
    " <br>&emsp;[34, 417, 877, 166, 213, 517]]</li><br>\n",
    "<li> Step-3: Pre-Padding the sequences to make them of equal lengths.\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,   0,   0,  34, 417],\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,   0,  34, 417, 877],\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,  34, 417, 877, 166],\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,  34, 417, 877, 166, 213],\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,  34, 417, 877, 166, 213, 517]</li><br>\n",
    "<li> Step-4: Final training sequences and labels.\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,   0,   0,  34, 417],&emsp; 417\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,   0,  34, 417, 877],&emsp; 877\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,   0,  34, 417, 877, 166],&emsp; 166\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,   0,  34, 417, 877, 166, 213],&emsp; 213\n",
    "<br>&emsp;[  0,   0,   0,   0,   0,  34, 417, 877, 166, 213, 517] &emsp; 517</li><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data using a word Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_data)\n",
    "tokenized_data = tokenizer.texts_to_sequences(raw_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n-grams\n",
    "training_sequences = [\n",
    "    sequence[:i] for sequence in tokenized_data for i in range(2, len(sequence) + 1)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the data with zeros\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(training_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training data\n",
    "x_train, y_train = padded_sequences[:, :-1], padded_sequences[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Defining and Training the Model</h3>\n",
    "We are using a Sequential model with the following layers:\n",
    "\n",
    "1. Embedding\n",
    "2. Bidirectional LSTM\n",
    "3. Dropout\n",
    "4. LSTM\n",
    "5. Dense\n",
    "6. Dense\n",
    "\n",
    "The parameters used are:\n",
    "<li> vocab_size=len(tokenizer.word_index)+1\n",
    "<li> wv_dims=100\n",
    "<li> input_length=x_train.shape[1]\n",
    "<li> dropout_rate=0.2\n",
    "<li> lstm_units=128\n",
    "<li> regularization_rate=0.01\n",
    "<li> epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for model with Bidirectional LSTM and LSTM layers\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "wv_dims = 100\n",
    "input_length = x_train.shape[1]\n",
    "dropout_rate = 0.2\n",
    "lstm_units = 128\n",
    "regularization_rate = 0.01\n",
    "epochs = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 10, 100)           321100    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 10, 256)          234496    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 256)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 802)               103458    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3211)              2578433   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,434,607\n",
      "Trainable params: 3,434,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, wv_dims, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(\n",
    "    Dense(\n",
    "        vocab_size // 4,\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=regularizers.L2(regularization_rate),\n",
    "    )\n",
    ")\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"../Writing_like_shakespeare/sm_LSTM.weights.best.hdf5\",\n",
    "    monitor=\"accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 7.1215 - accuracy: 0.0234\n",
      "Epoch 00001: accuracy improved from -inf to 0.02345, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 21s 150ms/step - loss: 7.1215 - accuracy: 0.0234 - val_loss: 6.8486 - val_accuracy: 0.0162\n",
      "Epoch 2/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 6.5913 - accuracy: 0.0233\n",
      "Epoch 00002: accuracy did not improve from 0.02345\n",
      "97/97 [==============================] - 14s 141ms/step - loss: 6.5913 - accuracy: 0.0233 - val_loss: 6.8655 - val_accuracy: 0.0184\n",
      "Epoch 3/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 6.4557 - accuracy: 0.0268\n",
      "Epoch 00003: accuracy improved from 0.02345 to 0.02676, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 6.4557 - accuracy: 0.0268 - val_loss: 6.8792 - val_accuracy: 0.0207\n",
      "Epoch 4/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 6.3099 - accuracy: 0.0320\n",
      "Epoch 00004: accuracy improved from 0.02676 to 0.03202, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 14s 142ms/step - loss: 6.3099 - accuracy: 0.0320 - val_loss: 6.9232 - val_accuracy: 0.0213\n",
      "Epoch 5/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 6.1899 - accuracy: 0.0361\n",
      "Epoch 00005: accuracy improved from 0.03202 to 0.03614, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 14s 141ms/step - loss: 6.1899 - accuracy: 0.0361 - val_loss: 7.0213 - val_accuracy: 0.0242\n",
      "Epoch 6/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 6.0545 - accuracy: 0.0420\n",
      "Epoch 00006: accuracy improved from 0.03614 to 0.04204, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 14s 141ms/step - loss: 6.0545 - accuracy: 0.0420 - val_loss: 7.0808 - val_accuracy: 0.0223\n",
      "Epoch 7/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.9065 - accuracy: 0.0472\n",
      "Epoch 00007: accuracy improved from 0.04204 to 0.04721, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 5.9065 - accuracy: 0.0472 - val_loss: 7.1765 - val_accuracy: 0.0281\n",
      "Epoch 8/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.7452 - accuracy: 0.0538\n",
      "Epoch 00008: accuracy improved from 0.04721 to 0.05384, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 14s 141ms/step - loss: 5.7452 - accuracy: 0.0538 - val_loss: 7.2422 - val_accuracy: 0.0275\n",
      "Epoch 9/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.5667 - accuracy: 0.0639\n",
      "Epoch 00009: accuracy improved from 0.05384 to 0.06387, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 153ms/step - loss: 5.5667 - accuracy: 0.0639 - val_loss: 7.4087 - val_accuracy: 0.0278\n",
      "Epoch 10/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.3794 - accuracy: 0.0724\n",
      "Epoch 00010: accuracy improved from 0.06387 to 0.07236, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 168ms/step - loss: 5.3794 - accuracy: 0.0724 - val_loss: 7.5962 - val_accuracy: 0.0346\n",
      "Epoch 11/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.1923 - accuracy: 0.0830\n",
      "Epoch 00011: accuracy improved from 0.07236 to 0.08303, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 167ms/step - loss: 5.1923 - accuracy: 0.0830 - val_loss: 7.7254 - val_accuracy: 0.0343\n",
      "Epoch 12/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 5.0152 - accuracy: 0.0936\n",
      "Epoch 00012: accuracy improved from 0.08303 to 0.09362, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 5.0152 - accuracy: 0.0936 - val_loss: 7.8256 - val_accuracy: 0.0359\n",
      "Epoch 13/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 4.8305 - accuracy: 0.1079\n",
      "Epoch 00013: accuracy improved from 0.09362 to 0.10793, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 161ms/step - loss: 4.8305 - accuracy: 0.1079 - val_loss: 8.1359 - val_accuracy: 0.0388\n",
      "Epoch 14/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 4.6504 - accuracy: 0.1199\n",
      "Epoch 00014: accuracy improved from 0.10793 to 0.11990, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 4.6504 - accuracy: 0.1199 - val_loss: 8.1027 - val_accuracy: 0.0365\n",
      "Epoch 15/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 4.4672 - accuracy: 0.1367\n",
      "Epoch 00015: accuracy improved from 0.11990 to 0.13671, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 4.4672 - accuracy: 0.1367 - val_loss: 8.4634 - val_accuracy: 0.0327\n",
      "Epoch 16/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 4.2725 - accuracy: 0.1598\n",
      "Epoch 00016: accuracy improved from 0.13671 to 0.15975, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 4.2725 - accuracy: 0.1598 - val_loss: 8.8811 - val_accuracy: 0.0349\n",
      "Epoch 17/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 4.0790 - accuracy: 0.1885\n",
      "Epoch 00017: accuracy improved from 0.15975 to 0.18846, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 160ms/step - loss: 4.0790 - accuracy: 0.1885 - val_loss: 8.9589 - val_accuracy: 0.0359\n",
      "Epoch 18/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 3.8877 - accuracy: 0.2195\n",
      "Epoch 00018: accuracy improved from 0.18846 to 0.21950, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 3.8877 - accuracy: 0.2195 - val_loss: 9.2047 - val_accuracy: 0.0349\n",
      "Epoch 19/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 3.6937 - accuracy: 0.2560\n",
      "Epoch 00019: accuracy improved from 0.21950 to 0.25596, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 3.6937 - accuracy: 0.2560 - val_loss: 9.5125 - val_accuracy: 0.0323\n",
      "Epoch 20/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 3.5017 - accuracy: 0.2963\n",
      "Epoch 00020: accuracy improved from 0.25596 to 0.29631, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 155ms/step - loss: 3.5017 - accuracy: 0.2963 - val_loss: 9.7364 - val_accuracy: 0.0336\n",
      "Epoch 21/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 3.3142 - accuracy: 0.3372\n",
      "Epoch 00021: accuracy improved from 0.29631 to 0.33721, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 165ms/step - loss: 3.3142 - accuracy: 0.3372 - val_loss: 10.1230 - val_accuracy: 0.0343\n",
      "Epoch 22/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 3.1367 - accuracy: 0.3823\n",
      "Epoch 00022: accuracy improved from 0.33721 to 0.38233, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 3.1367 - accuracy: 0.3823 - val_loss: 10.3490 - val_accuracy: 0.0320\n",
      "Epoch 23/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.9717 - accuracy: 0.4228\n",
      "Epoch 00023: accuracy improved from 0.38233 to 0.42275, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 168ms/step - loss: 2.9717 - accuracy: 0.4228 - val_loss: 10.3946 - val_accuracy: 0.0375\n",
      "Epoch 24/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.8057 - accuracy: 0.4633\n",
      "Epoch 00024: accuracy improved from 0.42275 to 0.46334, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 166ms/step - loss: 2.8057 - accuracy: 0.4633 - val_loss: 10.6439 - val_accuracy: 0.0320\n",
      "Epoch 25/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.6470 - accuracy: 0.4978\n",
      "Epoch 00025: accuracy improved from 0.46334 to 0.49778, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 155ms/step - loss: 2.6470 - accuracy: 0.4978 - val_loss: 10.8228 - val_accuracy: 0.0362\n",
      "Epoch 26/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.5030 - accuracy: 0.5397\n",
      "Epoch 00026: accuracy improved from 0.49778 to 0.53974, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 2.5030 - accuracy: 0.5397 - val_loss: 10.9247 - val_accuracy: 0.0314\n",
      "Epoch 27/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.3609 - accuracy: 0.5743\n",
      "Epoch 00027: accuracy improved from 0.53974 to 0.57426, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 2.3609 - accuracy: 0.5743 - val_loss: 11.0797 - val_accuracy: 0.0317\n",
      "Epoch 28/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.2303 - accuracy: 0.6087\n",
      "Epoch 00028: accuracy improved from 0.57426 to 0.60870, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 2.2303 - accuracy: 0.6087 - val_loss: 11.2026 - val_accuracy: 0.0307\n",
      "Epoch 29/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 2.1061 - accuracy: 0.6407\n",
      "Epoch 00029: accuracy improved from 0.60870 to 0.64071, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 2.1061 - accuracy: 0.6407 - val_loss: 11.1835 - val_accuracy: 0.0301\n",
      "Epoch 30/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.9972 - accuracy: 0.6722\n",
      "Epoch 00030: accuracy improved from 0.64071 to 0.67216, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 1.9972 - accuracy: 0.6722 - val_loss: 11.1515 - val_accuracy: 0.0288\n",
      "Epoch 31/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.8887 - accuracy: 0.6951\n",
      "Epoch 00031: accuracy improved from 0.67216 to 0.69512, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.8887 - accuracy: 0.6951 - val_loss: 11.3999 - val_accuracy: 0.0327\n",
      "Epoch 32/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.7868 - accuracy: 0.7187\n",
      "Epoch 00032: accuracy improved from 0.69512 to 0.71865, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 162ms/step - loss: 1.7868 - accuracy: 0.7187 - val_loss: 11.5246 - val_accuracy: 0.0304\n",
      "Epoch 33/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.6934 - accuracy: 0.7382\n",
      "Epoch 00033: accuracy improved from 0.71865 to 0.73822, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.6934 - accuracy: 0.7382 - val_loss: 11.5561 - val_accuracy: 0.0252\n",
      "Epoch 34/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.6188 - accuracy: 0.7563\n",
      "Epoch 00034: accuracy improved from 0.73822 to 0.75633, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 1.6188 - accuracy: 0.7563 - val_loss: 11.4442 - val_accuracy: 0.0310\n",
      "Epoch 35/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.5418 - accuracy: 0.7706\n",
      "Epoch 00035: accuracy improved from 0.75633 to 0.77056, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 162ms/step - loss: 1.5418 - accuracy: 0.7706 - val_loss: 11.5649 - val_accuracy: 0.0288\n",
      "Epoch 36/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.4685 - accuracy: 0.7862\n",
      "Epoch 00036: accuracy improved from 0.77056 to 0.78616, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 159ms/step - loss: 1.4685 - accuracy: 0.7862 - val_loss: 11.5407 - val_accuracy: 0.0297\n",
      "Epoch 37/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.4036 - accuracy: 0.7978\n",
      "Epoch 00037: accuracy improved from 0.78616 to 0.79780, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 1.4036 - accuracy: 0.7978 - val_loss: 11.4254 - val_accuracy: 0.0291\n",
      "Epoch 38/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.3499 - accuracy: 0.8055\n",
      "Epoch 00038: accuracy improved from 0.79780 to 0.80548, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.3499 - accuracy: 0.8055 - val_loss: 11.4522 - val_accuracy: 0.0281\n",
      "Epoch 39/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.2966 - accuracy: 0.8157\n",
      "Epoch 00039: accuracy improved from 0.80548 to 0.81575, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 159ms/step - loss: 1.2966 - accuracy: 0.8157 - val_loss: 11.4616 - val_accuracy: 0.0275\n",
      "Epoch 40/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.2505 - accuracy: 0.8250\n",
      "Epoch 00040: accuracy improved from 0.81575 to 0.82505, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 1.2505 - accuracy: 0.8250 - val_loss: 11.5218 - val_accuracy: 0.0278\n",
      "Epoch 41/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.2110 - accuracy: 0.8276\n",
      "Epoch 00041: accuracy improved from 0.82505 to 0.82763, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.2110 - accuracy: 0.8276 - val_loss: 11.4097 - val_accuracy: 0.0291\n",
      "Epoch 42/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.1679 - accuracy: 0.8334\n",
      "Epoch 00042: accuracy improved from 0.82763 to 0.83337, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 158ms/step - loss: 1.1679 - accuracy: 0.8334 - val_loss: 11.3611 - val_accuracy: 0.0272\n",
      "Epoch 43/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.1330 - accuracy: 0.8381\n",
      "Epoch 00043: accuracy improved from 0.83337 to 0.83806, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 159ms/step - loss: 1.1330 - accuracy: 0.8381 - val_loss: 11.4385 - val_accuracy: 0.0301\n",
      "Epoch 44/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.8404\n",
      "Epoch 00044: accuracy improved from 0.83806 to 0.84041, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 159ms/step - loss: 1.0972 - accuracy: 0.8404 - val_loss: 11.3166 - val_accuracy: 0.0255\n",
      "Epoch 45/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.0675 - accuracy: 0.8449\n",
      "Epoch 00045: accuracy improved from 0.84041 to 0.84493, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.0675 - accuracy: 0.8449 - val_loss: 11.3782 - val_accuracy: 0.0268\n",
      "Epoch 46/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.0386 - accuracy: 0.8474\n",
      "Epoch 00046: accuracy improved from 0.84493 to 0.84744, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 1.0386 - accuracy: 0.8474 - val_loss: 11.2945 - val_accuracy: 0.0281\n",
      "Epoch 47/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 1.0130 - accuracy: 0.8495\n",
      "Epoch 00047: accuracy improved from 0.84744 to 0.84954, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 16s 163ms/step - loss: 1.0130 - accuracy: 0.8495 - val_loss: 11.2326 - val_accuracy: 0.0291\n",
      "Epoch 48/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.8497\n",
      "Epoch 00048: accuracy improved from 0.84954 to 0.84970, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 155ms/step - loss: 0.9920 - accuracy: 0.8497 - val_loss: 11.2440 - val_accuracy: 0.0275\n",
      "Epoch 49/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.9693 - accuracy: 0.8519\n",
      "Epoch 00049: accuracy improved from 0.84970 to 0.85189, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 157ms/step - loss: 0.9693 - accuracy: 0.8519 - val_loss: 11.2554 - val_accuracy: 0.0294\n",
      "Epoch 50/50\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 0.8522\n",
      "Epoch 00050: accuracy improved from 0.85189 to 0.85221, saving model to ../Writing_like_shakespeare\\sm_LSTM.weights.best.hdf5\n",
      "97/97 [==============================] - 15s 156ms/step - loss: 0.9472 - accuracy: 0.8522 - val_loss: 11.2753 - val_accuracy: 0.0262\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76322c893dab8b09142fd7bae0c6d93651af8b59effcf127727af7ee52d1bae0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
